# Model Validation Guide

This document details how to use the system's built-in model validation features.

## Comparisons Overview

The validation function (`validate_federated_model.py`) provides a complete analysis of Federated Learning model performance, supporting:

- ✅ **Multi-Model Comparison**: Automatically compares the baseline model with aggregated models generated in each round of the experiment.
- ✅ **Detailed Metrics Comparison**: Provides mAP@0.5, mAP@0.5:0.95, Precision, and Recall.
- ✅ **Per-Class Analysis**: Displays the AP (Average Precision) value for each category in the dataset, facilitating observation of the model's learning effectiveness for specific categories.
- ✅ **Dataset Analysis**: Detects the class distribution of the dataset and marks categories missing in model predictions.
- ✅ **Performance Comparison**: Quantitatively calculates the performance improvement (percentage) of each round's model relative to the baseline model.

## How to Enable Validation

### Validation in Automated Flow
When running the `orchestrate.sh` automated script, simply add the `--val` flag to automatically trigger the complete validation flow after all Federated Learning rounds end.

```bash
# Example: Run a kitti experiment with 4 clients, 2 rounds, and validate afterwards
./src/orchestrate.sh kitti 4 2 --val
```

### Including Validation in Manual Mode (SOP)
If you want the generated manual execution script (`sop.sh`) to include validation steps, add the `--val` flag in the generation command as well.

```bash
# Example: Generate an SOP script including validation steps
./src/orchestrate.sh kitti 4 2 --manual --val > sop.sh
```

## Manual Validation Execution

You can also run the validation script independently for any completed experiment at any time.

### Basic Syntax
```bash
python3 src/validate_federated_model.py \
    --experiment-dir <path_to_experiment_results> \
    --data-config <path_to_dataset_config>
```

### Example
```bash
# Validate all models (baseline + all rounds) in a specific experiment
python3 src/validate_federated_model.py \
    --experiment-dir experiments/9_kitti_fedavg_4C_2R_202508051443 \
    --data-config data/kitti.yaml
```

### Custom Options
You can also specify a baseline model or output directory for more flexible comparisons.
```bash
# Validation using custom baseline model and output directory
python3 src/validate_federated_model.py \
    --experiment-dir experiments/9_kitti_fedavg_4C_2R_202508051443 \
    --data-config data/cityscapes.yaml \
    --baseline-model /path/to/your/custom_baseline.pt \
    --output-dir /path/to/your/custom_validation_results
```

## Validation Results Explanation

After validation is complete, all results will be saved in the `validation/` folder under the experiment directory (`<experiment_path>/validation/`).

### Output File Structure
- **`validation_summary.txt`**: A human-readable plaintext summary report containing all important comparison tables.
- **`model_comparison.json`**: Complete data in JSON format, facilitating subsequent processing or plotting by programs.
- **`validation_{model_name}/`**: Independent validation result folders for each model (baseline, round_1, etc.), containing detailed validation logs and charts generated by YOLOv9.

### How to Interpret `validation_summary.txt`
This report is key to quickly understanding model performance.

**Report Example:**
```
### Federated Learning Model Validation Summary ###

# 1. Overall Metrics Comparison Table
Model           mAP@0.5    mAP@0.75   mAP@0.5:0.95 Precision  Recall    
--------------------------------------------------------------------------------
baseline        0.752      0.543      0.481        0.743      0.687     
round_1         0.768      0.558      0.495        0.751      0.702     
round_2         0.784      0.572      0.508        0.762      0.718     

# 2. Performance Improvement Analysis
=== Performance Analysis ===
Baseline Model (baseline):
  mAP@0.5 = 0.752
  mAP@0.5:0.95 = 0.481

round_2:
  mAP@0.5 = 0.784 (Δ = +0.032, +4.3%)
  mAP@0.5:0.95 = 0.508 (Δ = +0.027, +5.6%)
```
From the example, it is clearly seen that after two rounds of Federated Learning, `mAP@0.5` improved by 4.3% compared to the baseline model.
