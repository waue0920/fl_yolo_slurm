#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --partition=gp1d      ## Partition: gtest | gp2d
#SBATCH --account="GOV113038"  ## iService_ID Project ID
#SBATCH --job-name=fl_yolo_slurm
#SBATCH --output=experiments/slurm.out
#SBATCH --error=experiments/slurm.out

# ===================================================================================
# Orchestrator Slurm Script for Federated Learning Experiments (Auto Mode)
# ===================================================================================
# Usage: sbatch src/run.sb <DATASET_NAME> <CLIENT_NUM> <TOTAL_ROUNDS> [--val]
# ex :
# sbatch src/run.sb kitti 4 5
# sbatch src/run.sb cityscapes 4 5 --val
# ===================================================================================

export WROOT="/home/waue0920/waue/git/fl_yolo_slurm"

set -e
set -o pipefail

### Logic Explanation
## (Login Node ln)
## 1. Execute sbatch run.sb
## 2. Request a worker node w1 from Slurm
## (Worker Node w1)
## 3. On worker node, check dataset partitioning
## 4. Start loop for each round
##   4.1 Logic of client_train.sh: Assuming 4 clients, submit 4 sbatch train jobs in parallel from w1
##       Assuming each sbatch job requests one node, i.e., {w2, w3, w4, w5}
##   4.2 (Worker Node w2) c1 train; (w3) c2 train; (w4) c3 train; (w5) c4 train
##   4.3 (Worker Node w1) Waits for all jobs to finish, then starts FL aggregation.
## 5. End of this round
## 6. Check if validation runs...
### End Explanation

# --- 1. Argument Parsing ---
VALIDATION_ENABLED=false
args=("$@")
filtered_args=()
for arg in "${args[@]}"; do
    if [[ "$arg" == "--val" ]]; then
        VALIDATION_ENABLED=true
    else
        filtered_args+=("$arg")
    fi
done
set -- "${filtered_args[@]}"

if [ "$#" -ne 3 ]; then
    echo "Usage: sbatch src/run.sb <DATASET_NAME> <CLIENT_NUM> <TOTAL_ROUNDS> [--val]"
    exit 1
fi
DATASET_NAME=$1
CLIENT_NUM=$2
TOTAL_ROUNDS=$3
INITIAL_WEIGHTS="yolov9-c.pt"
EXTRA_ARGS="--epochs 50 --batch 8"  # 可根據需求調整

SRC_DIR="${WROOT}/src"

cd "${WROOT}"

# --- 2. Generate Experiment ID ---
EXPERIMENTS_BASE_DIR="experiments"
SINGULARITY_IMG="${WROOT}/yolo9t2_ngc2306_20241226.sif"
mkdir -p ${EXPERIMENTS_BASE_DIR}
RUN_COUNT=$(find "${EXPERIMENTS_BASE_DIR}" -maxdepth 1 -type d | wc -l)
RUN_NUM=$((RUN_COUNT))
TIMESTAMP=$(date +%Y%m%d%H%M)
EXP_ID="${RUN_NUM}_${DATASET_NAME}_${CLIENT_NUM}C_${TOTAL_ROUNDS}R_${TIMESTAMP}"
EXP_DIR="${EXPERIMENTS_BASE_DIR}/${EXP_ID}"

# --- 3. Create Directories ---
mkdir -p "${EXP_DIR}/slurm_logs"
mkdir -p "${EXP_DIR}/client_outputs/${EXP_ID}"
mkdir -p "${EXP_DIR}/aggregated_weights"
mkdir -p "${EXP_DIR}/fed_avg_logs"

# --- 4. Setup Logging ---
exec > >(tee -a "${EXP_DIR}/orchestrator.log")
exec 2> >(tee -a "${EXP_DIR}/orchestrator.log" >&2)

echo "######################################################################"
echo "##  STARTING NEW PARALLEL FEDERATED LEARNING EXPERIMENT (SLURM AUTO)"
echo "##  Experiment ID: ${EXP_ID}"
echo "######################################################################"

echo -e "\n--- STEP 1: Preparing data for ${CLIENT_NUM} clients... ---"
##################
# FL 1 Dataset Partitioning
##################
# Must run in container to avoid missing python libs on worker node
singularity exec --nv --bind "${WROOT}":"${WROOT}" \
    "${SINGULARITY_IMG}" \
    python3 "${SRC_DIR}/data_prepare.py" --dataset-name "${DATASET_NAME}" \
    --num-clients "${CLIENT_NUM}"
echo "--- Data preparation step complete. ---"

# Ensure loop logic for ROUND is correct
echo -e "\n--- STEP 2: Starting Federated Learning Rounds... ---"
for r in $(seq 1 ${TOTAL_ROUNDS}); do
    echo -e "\n==================[ ROUND ${r} / ${TOTAL_ROUNDS} ]=================="
    if [ "${r}" -eq 1 ]; then
        current_weights="${WROOT}/${INITIAL_WEIGHTS}"
    else
        prev_round=$((r - 1))
        current_weights="${WROOT}/${EXP_DIR}/aggregated_weights/w_s_r${prev_round}.pt"
    fi
    echo "Using weights for this round: ${current_weights}"

    ##################
    # FL 2 Client Side Distributed Training
    ##################
    client_job_ids=""
    for c in $(seq 1 ${CLIENT_NUM}); do
        DATA_YAML="federated_data/${DATASET_NAME}_${CLIENT_NUM}/c${c}.yaml"
        WEIGHTS_IN="${current_weights}"
        PROJECT_OUT="${EXP_DIR}/client_outputs/${EXP_ID}"
        NAME_OUT="r${r}_c${c}"

        # [Error Example]
        # Tried using sbatch --wrap to submit client job, but python3 was not found
        # Shows FATAL: python3: executable file not found in $PATH
        # [Correct Usage] Switch back to sbatch executing client_train.sh to submit client job
        echo "Using weights for this round: ${current_weights}"

        # Submit all client jobs for the current round in parallel
        # The fl_client.sh script returns a list of submitted job IDs
        output=$("${SRC_DIR}/fl_client_train.sh" \
            "${WROOT}/${EXP_DIR}" \
            "${WROOT}" \
            "${r}" \
            "${CLIENT_NUM}" \
            "${DATASET_NAME}" \
            "${current_weights}")

        # Extract job IDs from the output of fl_client.sh
        # Assuming sbatch output is "Submitted batch job XXXXXX"
        client_job_ids=$(echo "${output}" | grep "Submitted batch job" | awk '{print $4}' | tr '\n' ' ')
        
        # Check if we have job IDs
        if [ -z "${client_job_ids}" ]; then
            echo "Error: Failed to submit client jobs or parse job IDs for round ${r}. Exiting." >&2
            exit 1
        fi

        dependency_list=$(echo ${client_job_ids} | sed 's/ /:/g')
        echo -e "\n--> All client jobs submitted. Dependency list: ${dependency_list}"
        echo ">> Submitting federated averaging job, which will run after clients finish."
    done

    dependency_list=$(echo ${client_job_ids} | sed 's/ /:/g')
    echo -e "\n--> All client jobs submitted. Dependency list: ${dependency_list}"
    echo ">> Submitting federated averaging job, which will run after clients finish."

    ##################
    # FL 3 Server Side Aggregation
    ##################
    # [Error Example] This causes server aggregation not to run in Singularity container,
    # leading to missing Python modules (like torch):
    # srun --dependency=afterok:${dependency_list} \
    #     python3 "${SRC_DIR}/server_fedavg.py" \
    #     ...
    # [Error Usage] Wrapping in singularity exec below ensures execution in container, but doesn't guarantee waiting for client job...
    # srun --dependency=afterok:${dependency_list} \
    #     singularity exec --nv --bind "${WROOT}":"${WROOT}" \
    #     ...
    # [Correct Usage] Should use sbatch to submit a new dependent job, and use --wait to wait for it.
    # This keeps run.sb orchestrator job paused until aggregation job finishes.
    "${SRC_DIR}/fl_server_fedavg.sh" \
        "${WROOT}/${EXP_DIR}" \
        "${WROOT}" \
        "${r}" \
        "${CLIENT_NUM}" \
        --dependency "${dependency_list}" \
        --wait

    echo "--> Federated averaging for Round ${r} complete."

done


##################
# FL 4 Server Side Validation
##################

if [ "$VALIDATION_ENABLED" = true ]; then
    echo -e "\n--- STEP 3: Running Model Validation ---"
    echo ">> Validating all models (baseline + ${TOTAL_ROUNDS} rounds)..."
    # [Error Example] This causes validation not to run in Singularity container,
    # leading to missing Python modules (like torch):
    # if python3 "${SRC_DIR}/validate_federated_model.py" \
    #     --experiment-dir "${WROOT}/${EXP_DIR}" \
    #     --data-config "data/${DATASET_NAME}.yaml" \
    #     ; then
    #     echo "--- Model validation complete. ---"
    #     VALIDATION_MSG="##  Validation results: ${EXP_DIR}/validation/"
    # else
    #     echo "Warning: Model validation failed, but experiment completed successfully."
    #     VALIDATION_MSG="##  Validation: Failed (see logs above)"
    # fi

    # [Correct Example] Wrap in singularity exec to ensure execution in container:
    if singularity exec --nv --bind "${WROOT}":"${WROOT}" \
        "${SINGULARITY_IMG}" \
        python3 "${WROOT}/src/validate_federated_model.py" \
        --experiment-dir "${WROOT}/${EXP_DIR}" \
        --data-config "${WROOT}/data/${DATASET_NAME}.yaml" \
        ; then
        echo "--- Model validation complete. ---"
        VALIDATION_MSG="##  Validation results: ${EXP_DIR}/validation/"
    else
        echo "Warning: Model validation failed, but experiment completed successfully."
        VALIDATION_MSG="##  Validation: Failed (see logs above)"
    fi
else
    VALIDATION_MSG="##  Validation: Skipped (use --val to enable)"
fi

echo -e "\n######################################################################"
echo "##  AUTOMATED FEDERATED LEARNING EXPERIMENT COMPLETED"
echo "##  Final model: ${EXP_DIR}/aggregated_weights/w_s_r${TOTAL_ROUNDS}.pt"
echo "${VALIDATION_MSG}"
echo "######################################################################"
